

# 邏輯回歸的概念

**logistic回归**是一种广义线性回归（generalized linear model），因此与多重线性回归分析有很多相同之处。它们的模型形式基本上相同，都具有 w‘x+b，其中w和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将w‘x+b作为因变量，即y =w‘x+b，而logistic回归则通过函数L将w‘x+b对应一个隐状态p，p =L(w‘x+b),然后根据p 与1-p的大小决定因变量的值。如果L是logistic函数，就是logistic回归，如果L是多项式函数就是多项式回归。

l**ogistic回归**的因变量可以是**二分类**的，也可以是多分类的，但是二分类的更为常用，也更加容易解释，多类可以使用softmax方法进行处理。**实际中最为常用的就是二分类的logistic回归**

### Logistic回归模型的适用条件：
1.  因变量为二分类的分类变量或某事件的发生率，并且是数值型变量。但是需要注意，重复计数现象指标不适用于Logistic回归。

2.  残差和因变量都要服从**二项分布**。二项分布对应的是分类变量，所以不是正态分布，进而不是用最小二乘法，而是最大似然法来解决方程估计和检验问题。
3.  自变量和Logistic概率是线性关系
4. 各观测对象间相互独立。

**原理**：如果直接将线性回归的模型扣到Logistic回归中，会造成方程二边取值区间不同和普遍的非直线关系。因为Logistic中因变量为二分类变量，某个概率作为方程的因变量估计值取值范围为0-1，但是，方程右边取值范围是无穷大或者无穷小。所以，才引入Logistic回归。

**Logistic回归实质**：发生概率除以没有发生概率再取对数。就是这个不太繁琐的变换改变了取值区间的矛盾和因变量自变量间的曲线关系。究其原因，是发生和未发生的概率成为了比值 ，这个比值就是一个缓冲，将取值范围扩大，再进行对数变换，整个因变量改变。不仅如此，这种变换往往使得因变量和自变量之间呈线性关系，这是根据大量实践而总结。所以，Logistic回归从根本上解决因变量要不是连续变量怎么办的问题。还有，Logistic应用广泛的原因是许多现实问题跟它的模型吻合。例如一件事情是否发生跟其他数值型自变量的关系。

---


# 逻辑回归方程
![请添加图片描述](https://i-blog.csdnimg.cn/blog_migrate/becfd72769350e4c0c6e7b0308d3c9a1.png)
![方程所对应的图形](https://i-blog.csdnimg.cn/blog_migrate/316f8d2afc212888ad93c05f40306de1.png)
可以发现当样本值越大y越接近1，样本值越小y越接近0，以0.5为分界点就可以将y分成以下两种情况。
![请添加图片描述](https://i-blog.csdnimg.cn/blog_migrate/fb76fdfdcfd8d916e862e93ad77cc998.png)

这时负样本就是0，正样本就是1，0和1就是我们给样本定义的标签。在考试通过问题中，可以用1代表通过（pass）,用0代表失败（failed），这样就可以通过标签0和1将失败和通过进行一个分类，就可以很好地解决二分类问题了。

当问题更复杂时，可以用g(x)代替x，然后就可以根据g(x)寻找最合适的决策边界从而解决二分类问题
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/1b8fb3f2b9407c870a2b165fe5dd4f9a.png)
比如下面这个图形：
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/48d23028e82b052fe3ea18eda591aa64.png)

此时虚线部分就是它的决策边界![请添加图片描述](https://i-blog.csdnimg.cn/blog_migrate/ecd42d3df579629b533d0d22d44cbb65.png)

![请添加图片描述](https://i-blog.csdnimg.cn/blog_migrate/7f9d569aa5c747561b013a8004146a1a.png)


# 实战
**任务:**基于examdata.csv数据，建立逻辑回归模型 预测Exam1 = 70, Exam2 = 60时，该同学在Exam3是 passed or failed; 建立二阶边界，提高模型准确度。

```python
#load the data
import pandas as pd
import numpy as np
data = pd.read_csv('examdata.csv')
data.head()
```

![](https://i-blog.csdnimg.cn/blog_migrate/9c489da6399e41cfc30a6e46ddcecc72.png)

```python
#visualize the data
from matplotlib import pyplot as plt
fig1 = plt.figure()
plt.scatter(data.loc[:,'Exam1'],data.loc[:,'Exam2'])
plt.title('Exam1-Exam2')
plt.xlabel('Exam1')
plt.ylabel('Exam2')
plt.show()
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/e975e0636c60cfb6c83f5d54fd83c8c7.png)



```python
#add label mask
mask=data.loc[:,'Pass']==1
print(mask)
```



![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/9be8673a97a9447addd68ceaf9358e03.png)

```python
fig2 = plt.figure()
passed=plt.scatter(data.loc[:,'Exam1'][mask],data.loc[:,'Exam2'][mask])
failed=plt.scatter(data.loc[:,'Exam1'][~mask],data.loc[:,'Exam2'][~mask])
plt.title('Exam1-Exam2')
plt.xlabel('Exam1')
plt.ylabel('Exam2')
plt.legend((passed,failed),('passed','failed'))
plt.show()
```


![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/9ae8bba00e931260cd3bab1ee13fc0b1.png)



```python
#define X,y
X = data.drop(['Pass'],axis=1)
y = data.loc[:,'Pass']
X1 = data.loc[:,'Exam1']
X2 = data.loc[:,'Exam2']
X1.head()
```
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/5ba9484de5fd8e9bb34a3cf9156c9b68.png)

```python
X.shape,y.shape #查看維度
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/7c6f98c541c55305de0a373d7c7e4d46.png)


```python
#establish the model and  train it
from sklearn.linear_model import LogisticRegression
LR = LogisticRegression()
LR.fit(X,y)
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/610c62225aeaede80f53cd3ee0f84926.png)

这里模型已经训练好了

```python
#show the predicted result and its accuracy
y_predict = LR.predict(X)
print(y_predict)
```
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/c916f451409f078941da1fc67729cea6.png)

```python
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y,y_predict) # accuracy_score分數越接近1越好 
accuracy
```
准确率达到了0.89，虽然感觉挺高的，但是判断错误的还是很多的


```python
#exam1=70,exam2=65
y_test = LR.predict([[70,65]])
print('passed' if y_test==1 else 'failed')
```
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/b16248f8af30c2440cfb1d7a28bd5e01.png)

### 邊界函數
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/3c96b51de986f822843eb2bcd5a1417a.png)


```python
LR.intercept_	#截距 也就是0次項的係數
```
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/e05e22ed68655dfec42c7e7e9e479e9e.png)

```python
theta0 = LR.intercept_
theta1,theta2 = LR.coef_[0][0],LR.coef_[0][1]
print(theta0,theta1,theta2)
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/9dac81f6ef4f1939dc7c8b1fe35ca1a5.png)

```python
X2_new = -(theta0 + theta1*X1) / theta2
print(X2_new)
```
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/f4a924d91a4aecacd3efc6ff6446cec4.png)

```python
fig3 = plt.figure()
passed=plt.scatter(data.loc[:,'Exam1'][mask],data.loc[:,'Exam2'][mask])
failed=plt.scatter(data.loc[:,'Exam1'][~mask],data.loc[:,'Exam2'][~mask])
plt.title('Exam1-Exam2')
plt.xlabel('Exam1')
plt.ylabel('Exam2')
plt.legend((passed,failed),('passed','failed'))
plt.plot(X1,X2_new)
plt.show()
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/4f3c04e36648efe126aa4ecf7d4fe466.png)

```python
# create new data
X1_2 = X1*X1
X2_2 = X2*X2
X1_X2 = X1*X2
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/33458e9edd9f565d4582a6779b452730.png)

```python
X_new = {'X1':X1,'X2':X2,'X1_2':X1_2,'X2_2':X2_2,'X1_X2':X1_X2}#将所有数据放到一个字典里面
X_new = pd.DataFrame(X_new) #方便后面进行模型数据的加载
print(X_new)
```
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/64982d06011652571afa5a5a59b036dc.png)

訓練新模型
```python
#establish new model and train
LR2 = LogisticRegression()
LR2.fit(X_new,y)
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/0b5770016ce2cd446bee1cbf5400fc1b.png)

```python
y2_predict = LR2.predict(X_new)
accuracy2 = accuracy_score(y,y2_predict)
print(accuracy2)
```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/0bc7d678ed7ad9fb8ff10053b89e33eb.png)

**accuracy_score**准确率达到了百分之百，可以看处通过二阶边界建立的模型比通过一阶边界建立的模型准确率高很多

### 二階邊界函數
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/a53845e6999b58a17716bf53f40c8e83.png)

```python
X1_new = X1.sort_values() #将x1_new从小到大排序,这里是保证后面画图不出现交叉，使图形更好看、更直观
print(X1_new) #预览
```
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/c001ae0de9e7d2038090ecefe755d768.png)
看一下可視化效果
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/4a743ca16dddd7337b9db8449bd740d5.png)
如果不排序的效果如下，可以看出很亂

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/ae482066df725a557f48266801c6f162.png)


```python
theta0 = LR2.intercept_
theta1,theta2,theta3,theta4,theta5 = LR2.coef_[0][0],LR2.coef_[0][1],LR2.coef_[0][2],LR2.coef_[0][3],LR2.coef_[0][4]
a = theta4
b = theta5*X1_new + theta2
c = theta0 + theta1*X1_new + theta3*X1_new*X1_new
X2_new_boundary = (-b + np.sqrt(b*b - 4*a*c))/(2*a)
print(X2_new_boundary)
```


![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/81e0115769d7c55042e76c4dfd34d19b.png)


```python
fig5 = plt.figure()
passed=plt.scatter(data.loc[:,'Exam1'][mask],data.loc[:,'Exam2'][mask])
failed=plt.scatter(data.loc[:,'Exam1'][~mask],data.loc[:,'Exam2'][~mask])
plt.plot(X1_new,X2_new_boundary) #画出决策边界
plt.title('Exam1-Exam2')
plt.xlabel('Exam1')
plt.ylabel('Exam2')
plt.legend((passed,failed),('passed','failed'))
plt.show()
```



<img width="635" alt="image" src="https://github.com/user-attachments/assets/c7378c70-5b01-4be6-b139-07a2ab86d6d1">



# 總結
以上就是本次跟著flare老師學習的筆記，附上数据集和源碼，需要的小伙伴自取
鏈接：[https://github.com/fbozhang/python/tree/master/jupyter](https://github.com/fbozhang/python/tree/master/jupyter)






